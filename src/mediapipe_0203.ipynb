{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM 모델 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization, Attention\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Data\n",
    "data = pd.read_csv(r'../data/keypoints_output_with_filename.csv')\n",
    "\n",
    "# Step 2: Preprocess Data\n",
    "# Remove 'fall' from 'image_fall_num' and convert to integer\n",
    "data['image_fall_num'] = data['image_fall_num'].str.replace('fall', '').astype(int)\n",
    "\n",
    "# Sort and group by 'image_fall_num'\n",
    "data = data.sort_values(by=['image_fall_num', 'image_num'])  # Assume 'frame_index' indicates temporal order\n",
    "groups = data.groupby('image_fall_num')\n",
    "\n",
    "# Extract sequences and labels\n",
    "sequences = []\n",
    "labels = []\n",
    "for _, group in groups:\n",
    "    # Select only numeric columns and handle non-numeric values\n",
    "    numeric_data = group.iloc[:, 2:-1].apply(pd.to_numeric, errors='coerce')  # Convert to numeric, NaN if not possible\n",
    "    numeric_data = numeric_data.fillna(0)  # Replace NaN with 0 or another strategy\n",
    "    sequences.append(numeric_data.values)\n",
    "    \n",
    "    # Store all labels in the group instead of just one\n",
    "    labels.extend(group['label'].values)  # Append all labels from the group\n",
    "\n",
    "# Padding sequences to the same length\n",
    "max_seq_length = max(len(seq) for seq in sequences)\n",
    "num_features = sequences[0].shape[1]\n",
    "\n",
    "padded_sequences = np.zeros((len(labels), max_seq_length, num_features))\n",
    "label_array = np.array(labels, dtype=int)\n",
    "\n",
    "idx = 0\n",
    "for i, seq in enumerate(sequences):\n",
    "    for _ in range(len(seq)):\n",
    "        padded_sequences[idx, :len(seq), :] = seq\n",
    "        idx += 1\n",
    "\n",
    "# Ensure at least two classes before applying SMOTE\n",
    "if len(np.unique(label_array)) < 2:\n",
    "    minority_class = 1 if 1 in label_array else 2\n",
    "    label_array = np.concatenate([label_array, np.full(5, minority_class)])  # Add missing class\n",
    "\n",
    "# Check class distribution before SMOTE\n",
    "print(\"Original class distribution:\", np.unique(label_array, return_counts=True))\n",
    "\n",
    "# Normalize the sequences\n",
    "scaler = MinMaxScaler()\n",
    "num_samples, seq_length, num_features = padded_sequences.shape\n",
    "reshaped_sequences = padded_sequences.reshape(-1, num_features)\n",
    "norm_sequences = scaler.fit_transform(reshaped_sequences)\n",
    "norm_sequences = norm_sequences.reshape(num_samples, seq_length, num_features)\n",
    "\n",
    "# Apply SMOTE BEFORE train-test split\n",
    "unique_classes = np.unique(label_array)\n",
    "if len(unique_classes) > 1:\n",
    "    smote = SMOTE(sampling_strategy={1: 4000}, random_state=42)\n",
    "    norm_sequences_reshaped = norm_sequences.reshape(num_samples, -1)  # 2D 변환\n",
    "    norm_sequences_resampled, labels_resampled = smote.fit_resample(norm_sequences_reshaped, label_array)\n",
    "    # Reshape back to 3D\n",
    "    num_samples_resampled = norm_sequences_resampled.shape[0]\n",
    "    norm_sequences_resampled = norm_sequences_resampled.reshape(num_samples_resampled, max_seq_length, num_features)\n",
    "    print(\"SMOTE applied.\")\n",
    "else:\n",
    "    print(\"SMOTE not applied as only one class is present in dataset\")\n",
    "    norm_sequences_resampled, labels_resampled = norm_sequences, label_array\n",
    "\n",
    "# Train-test split AFTER SMOTE\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    norm_sequences_resampled, labels_resampled, test_size=0.2, random_state=42, stratify=labels_resampled\n",
    ")\n",
    "\n",
    "# Check class distribution in training data\n",
    "print(\"Training class distribution:\", np.unique(y_train, return_counts=True))\n",
    "\n",
    "# Data Augmentation: Add Time Noise\n",
    "\n",
    "def add_time_noise(data, noise_level=0.05):\n",
    "    noise = np.random.normal(loc=0, scale=noise_level, size=data.shape)\n",
    "    return data + noise\n",
    "\n",
    "X_train = np.array([add_time_noise(seq) for seq in X_train])\n",
    "print(\"Time noise added for data augmentation.\")\n",
    "\n",
    "# Convert labels back to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=3)\n",
    "y_test = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "# Step 3: Define the Model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(128, return_sequences=True, input_shape=(max_seq_length, num_features))),\n",
    "    Dropout(0.2),\n",
    "    BatchNormalization(),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.2),\n",
    "    BatchNormalization(),\n",
    "    LSTM(32, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 4: Train the Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the model\n",
    "model.save('fall_detection_model.keras')\n",
    "\n",
    "# 학습 곡선 시각화\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomOverSampler 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import legacy\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Data\n",
    "data = pd.read_csv(r'../data/keypoints_output_with_filename.csv')\n",
    "data['image_fall_num'] = data['image_fall_num'].str.replace('fall', '').astype(int)\n",
    "data = data.sort_values(by=['image_fall_num', 'image_num'])\n",
    "groups = data.groupby('image_fall_num')\n",
    "\n",
    "# Extract sequences and labels\n",
    "sequences = []\n",
    "labels = []\n",
    "for _, group in groups:\n",
    "    numeric_data = group.iloc[:, 2:-1].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    sequences.append(numeric_data.values)\n",
    "    labels.extend(group['label'].values)\n",
    "\n",
    "# Padding sequences\n",
    "max_seq_length = max(len(seq) for seq in sequences)\n",
    "num_features = sequences[0].shape[1]\n",
    "padded_sequences = np.zeros((len(labels), max_seq_length, num_features))\n",
    "label_array = np.array(labels, dtype=int)\n",
    "\n",
    "idx = 0\n",
    "for i, seq in enumerate(sequences):\n",
    "    for _ in range(len(seq)):\n",
    "        padded_sequences[idx, :len(seq), :] = seq\n",
    "        idx += 1\n",
    "\n",
    "# Normalize the sequences\n",
    "scaler = StandardScaler()\n",
    "num_samples, seq_length, num_features = padded_sequences.shape\n",
    "reshaped_sequences = padded_sequences.reshape(-1, num_features)\n",
    "norm_sequences = scaler.fit_transform(reshaped_sequences)\n",
    "norm_sequences = norm_sequences.reshape(num_samples, seq_length, num_features)\n",
    "\n",
    "# Apply RandomOverSampler BEFORE train-test split\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "norm_sequences_reshaped = norm_sequences.reshape(num_samples, -1)\n",
    "norm_sequences_resampled, labels_resampled = ros.fit_resample(norm_sequences_reshaped, label_array)\n",
    "norm_sequences_resampled = norm_sequences_resampled.reshape(len(labels_resampled), max_seq_length, num_features)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    norm_sequences_resampled, labels_resampled, test_size=0.2, random_state=42, stratify=labels_resampled\n",
    ")\n",
    "\n",
    "# Data Augmentation: Add Time Noise\n",
    "def add_time_noise(data, noise_level=0.05):\n",
    "    noise = np.random.normal(loc=0, scale=noise_level, size=data.shape)\n",
    "    return data + noise\n",
    "\n",
    "num_noisy_samples = int(len(X_train) * 0.5)\n",
    "X_train[:num_noisy_samples] = np.array([add_time_noise(seq) for seq in X_train[:num_noisy_samples]])\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=3)\n",
    "y_test = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "# Validate Data Types\n",
    "print(f\"X_train type: {X_train.dtype}, y_train type: {y_train.dtype}\")\n",
    "\n",
    "# Step 3: Define the Model\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(max_seq_length, num_features)),\n",
    "    Dropout(0.2),\n",
    "    BatchNormalization(),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    BatchNormalization(),\n",
    "    LSTM(32, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# 학습률 스케줄 적용\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate=0.001, decay_steps=10000, decay_rate=0.9, staircase=True)\n",
    "\n",
    "# AdamW 옵티마이저 설정 (legacy)\n",
    "optimizer = legacy.AdamW(learning_rate=lr_schedule)\n",
    "\n",
    "# 모델 다시 컴파일\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Confirm the loss function is passed correctly\n",
    "print(f\"Model compiled with loss: categorical_crossentropy\")\n",
    "\n",
    "# Step 4: Train the Model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the model\n",
    "model.save('fall_detection_model_1.keras')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakenews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
